
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# === Step 1: Generate or Load Dataset ===
np.random.seed(42)
num_samples = 1000

data = {
    "Disk_Usage": np.random.uniform(10, 100, num_samples),
    "Memory_Usage": np.random.uniform(10, 100, num_samples),
    "CPU_Load": np.random.uniform(5, 95, num_samples),
    "Network_Latency": np.random.uniform(5, 200, num_samples),
    "Packet_Drop_Rate": np.random.uniform(0, 20, num_samples),
    "Login_Attempts": np.random.randint(1, 10, num_samples),
    "Intrusion": np.random.choice([0, 1], size=num_samples, p=[0.9, 0.1])  # 10% intrusion cases
}

df = pd.DataFrame(data)

# Save dataset to CSV and Excel
df.to_csv("AI_Cloud_IDS_Dataset.csv", index=False)
df.to_excel("AI_Cloud_IDS_Dataset.xlsx", index=False)

print("\nDataset saved successfully! ")

# === Step 2: Data Preprocessing ===
X = df.drop(columns=["Intrusion"])
y = df["Intrusion"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# === Step 3: Model Training (Random Forest & XGBoost) ===
# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Train XGBoost (Fixed hyperparameters)
xgb_model = XGBClassifier(
    n_estimators=200,  
    learning_rate=0.05,  
    max_depth=6,  
    eval_metric="logloss",
    random_state=42
)
xgb_model.fit(X_train, y_train)

# === Step 4: Model Evaluation ===
rf_predictions = rf_model.predict(X_test)
xgb_predictions = xgb_model.predict(X_test)

rf_accuracy = accuracy_score(y_test, rf_predictions)
xgb_accuracy = accuracy_score(y_test, xgb_predictions)

print("\n Model Performance ")
print("\nRandom Forest Accuracy:", rf_accuracy)
print("\nXGBoost Accuracy:", xgb_accuracy)
# Fixed classification report warning
print("\nRandom Forest Classification Report:\n", classification_report(y_test, rf_predictions, zero_division=1))
print("\nXGBoost Classification Report:\n", classification_report(y_test, xgb_predictions, zero_division=1))

# === Step 5: Visualizations ===
# Confusion Matrix
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(confusion_matrix(y_test, rf_predictions), annot=True, fmt="d", cmap="Blues", xticklabels=["Normal", "Intrusion"], yticklabels=["Normal", "Intrusion"])
plt.title("Random Forest - Confusion Matrix")

plt.subplot(1, 2, 2)
sns.heatmap(confusion_matrix(y_test, xgb_predictions), annot=True, fmt="d", cmap="Greens", xticklabels=["Normal", "Intrusion"], yticklabels=["Normal", "Intrusion"])
plt.title("XGBoost - Confusion Matrix")
plt.show()
# Feature Importance
rf_importance = rf_model.feature_importances_
xgb_importance = xgb_model.feature_importances_
feature_names = df.columns[:-1]
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.barplot(x=rf_importance, y=feature_names)
plt.title("Random Forest Feature Importance")
plt.subplot(1, 2, 2)
sns.barplot(x=xgb_importance, y=feature_names)
plt.title("XGBoost Feature Importance")
plt.show()
print("\n Model training and evaluation completed! ")
